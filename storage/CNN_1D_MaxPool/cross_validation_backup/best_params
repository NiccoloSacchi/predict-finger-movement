L119547037146038801333356L
.L1001L
.(dp0
Vprotocol_version
p1
L1001L
sVlittle_endian
p2
I01
sVtype_sizes
p3
(dp4
Vshort
p5
L2L
sVint
p6
L4L
sVlong
p7
L4L
ss.(dp0
Vnb_hidden
p1
L40L
sVactivation
p2
P('module', <class 'torch.nn.modules.activation.ReLU'>, '/home/sacchi/anaconda3/lib/python3.6/site-packages/torch/nn/modules/activation.py', 'class ReLU(Threshold):\n    r"""Applies the rectified linear unit function element-wise\n    :math:`\\text{ReLU}(x)= \\max(0, x)`\n\n    .. image:: scripts/activation_images/ReLU.png\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, *)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(N, *)`, same shape as the input\n\n    Examples::\n\n        >>> m = nn.ReLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    """\n\n    def __init__(self, inplace=False):\n        super(ReLU, self).__init__(0, 0, inplace)\n\n    def extra_repr(self):\n        inplace_str = \'inplace\' if self.inplace else \'\'\n        return inplace_str\n')
sVoptimizer
p3
ctorch.optim.adamax
Adamax
p4
sVweight_decay
p5
F2.1544346900318823e-05
sVdropout
p6
F0.19999999999999998
s.(lp0
.