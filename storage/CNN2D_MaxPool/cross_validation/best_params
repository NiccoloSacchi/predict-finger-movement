L119547037146038801333356L
.L1001L
.(dp0
Vprotocol_version
p1
L1001L
sVlittle_endian
p2
I01
sVtype_sizes
p3
(dp4
Vshort
p5
L2L
sVint
p6
L4L
sVlong
p7
L4L
ss.(dp0
Vnb_layers
p1
L3L
sVnb_hidden
p2
L80L
sVactivation
p3
P('module', <class 'torch.nn.modules.activation.ELU'>, '/home/sacchi/anaconda3/lib/python3.6/site-packages/torch/nn/modules/activation.py', 'class ELU(Module):\n    r"""Applies element-wise,\n    :math:`\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))`\n\n    Args:\n        alpha: the :math:`\\alpha` value for the ELU formulation. Default: 1.0\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, *)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(N, *)`, same shape as the input\n\n    .. image:: scripts/activation_images/ELU.png\n\n    Examples::\n\n        >>> m = nn.ELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    """\n\n    def __init__(self, alpha=1., inplace=False):\n        super(ELU, self).__init__()\n        self.alpha = alpha\n        self.inplace = inplace\n\n    def forward(self, input):\n        return F.elu(input, self.alpha, self.inplace)\n\n    def extra_repr(self):\n        inplace_str = \', inplace\' if self.inplace else \'\'\n        return \'alpha={}{}\'.format(self.alpha, inplace_str)\n')
sVweight_decay
p4
F0.0024787521766663594
sVdropout
p5
F0.0
sVoptimizer
p6
ctorch.optim.adadelta
Adadelta
p7
s.(lp0
.