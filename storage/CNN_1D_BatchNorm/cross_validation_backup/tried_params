L119547037146038801333356L
.L1001L
.(dp0
Vprotocol_version
p1
L1001L
sVlittle_endian
p2
I01
sVtype_sizes
p3
(dp4
Vshort
p5
L2L
sVint
p6
L4L
sVlong
p7
L4L
ss.ccollections
OrderedDict
p0
(tRp1
Vnb_hidden
p2
(lp3
L40L
aL80L
aL120L
aL160L
aL200L
asVactivation
p4
(lp5
P('module', <class 'torch.nn.modules.activation.ReLU'>, '/home/sacchi/anaconda3/lib/python3.6/site-packages/torch/nn/modules/activation.py', 'class ReLU(Threshold):\n    r"""Applies the rectified linear unit function element-wise\n    :math:`\\text{ReLU}(x)= \\max(0, x)`\n\n    .. image:: scripts/activation_images/ReLU.png\n\n    Args:\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, *)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(N, *)`, same shape as the input\n\n    Examples::\n\n        >>> m = nn.ReLU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    """\n\n    def __init__(self, inplace=False):\n        super(ReLU, self).__init__(0, 0, inplace)\n\n    def extra_repr(self):\n        inplace_str = \'inplace\' if self.inplace else \'\'\n        return inplace_str\n')
aP('module', <class 'torch.nn.modules.activation.Tanh'>, '/home/sacchi/anaconda3/lib/python3.6/site-packages/torch/nn/modules/activation.py', 'class Tanh(Module):\n    r"""Applies element-wise,\n    :math:`\\text{Tanh}(x) = \\tanh(x) = \\frac{e^x - e^{-x}} {e^x + e^{-x}}`\n\n    Shape:\n        - Input: :math:`(N, *)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(N, *)`, same shape as the input\n\n    .. image:: scripts/activation_images/Tanh.png\n\n    Examples::\n\n        >>> m = nn.Tanh()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    """\n\n    def forward(self, input):\n        return torch.tanh(input)\n')
aP('module', <class 'torch.nn.modules.activation.ELU'>, '/home/sacchi/anaconda3/lib/python3.6/site-packages/torch/nn/modules/activation.py', 'class ELU(Module):\n    r"""Applies element-wise,\n    :math:`\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))`\n\n    Args:\n        alpha: the :math:`\\alpha` value for the ELU formulation. Default: 1.0\n        inplace: can optionally do the operation in-place. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, *)` where `*` means, any number of additional\n          dimensions\n        - Output: :math:`(N, *)`, same shape as the input\n\n    .. image:: scripts/activation_images/ELU.png\n\n    Examples::\n\n        >>> m = nn.ELU()\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    """\n\n    def __init__(self, alpha=1., inplace=False):\n        super(ELU, self).__init__()\n        self.alpha = alpha\n        self.inplace = inplace\n\n    def forward(self, input):\n        return F.elu(input, self.alpha, self.inplace)\n\n    def extra_repr(self):\n        inplace_str = \', inplace\' if self.inplace else \'\'\n        return \'alpha={}{}\'.format(self.alpha, inplace_str)\n')
asVoptimizer
p6
(lp7
ctorch.optim.adam
Adam
p8
actorch.optim.adadelta
Adadelta
p9
actorch.optim.adamax
Adamax
p10
asVweight_decay
p11
(lp12
F1e-06
aF2.1544346900318823e-05
aF0.00046415888336127773
aF0.01
asVdropout
p13
(lp14
F0.0
aF0.09999999999999999
aF0.19999999999999998
aF0.3
as.(lp0
.