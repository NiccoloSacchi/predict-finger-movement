{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1>Mini-project1</h1>\n",
    "    <h2>Predict the laterality of upcoming finger movements</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import dlc_bci as bci\n",
    "from types import SimpleNamespace \n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# baselines\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import torch \n",
    "from torch.nn import Conv1d, Conv2d, Linear, Module, CrossEntropyLoss\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from helpers import *\n",
    "from modelWrapper import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download/load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_khz=False\n",
    "\n",
    "train = SimpleNamespace()\n",
    "train.X, train.y = bci.load(root='./data_bci', one_khz=one_khz)\n",
    "print(str(type(train.X)), train.X.size())\n",
    "print(str(type(train.y)), train.y.size())\n",
    "\n",
    "test = SimpleNamespace()\n",
    "test.X, test.y = bci.load(root='./data_bci', train=False, one_khz=one_khz)\n",
    "print(str(type(test.X)), test.X.size())\n",
    "print(str(type(test.y)), test.y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "plt.imshow(X_tr[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the X (no standardization for random forest) \n",
    "X_tr, y_tr = train.X.view(train.X.shape[0], -1).clone().numpy(), train.y.numpy() \n",
    "X_te, y_te = test.X.view(test.X.shape[0], -1).clone().numpy(), test.y.numpy() \n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tune and compute test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(5, 100, 10) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different depths\n",
    "randForest = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "for depth in depths:\n",
    "    result = cross_validate(\n",
    "        RandomForestClassifier(n_estimators=100, max_depth=depth, n_jobs=-1, random_state=1), \n",
    "        X_tr, y_tr, cv=5, return_train_score=True)\n",
    "    \n",
    "    randForest[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    randForest[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(depths, \"depth\", randForest[\"tr_scores\"], randForest[\"va_scores\"], ylog_scale=False)\n",
    "\n",
    "best_depth = depths[np.argmax(randForest[\"va_scores\"])]\n",
    "print('Best depth:', best_depth)\n",
    "print('Test score:',\n",
    "      RandomForestClassifier(n_estimators=100, max_depth=depth, n_jobs=-1, random_state=1)\n",
    "      .fit(X_tr, y_tr)\n",
    "      .score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten and normalize the X\n",
    "X_tr, y_tr = train.X.view(train.X.shape[0], -1).clone().numpy(), train.y.numpy() \n",
    "X_te, y_te = test.X.view(test.X.shape[0], -1).clone().numpy(), test.y.numpy() \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_te = scaler.transform(X_te)\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tune and compute test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-6, 6, 10) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different lambdas\n",
    "logreg = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "# tune the model on the train set \n",
    "for lambda_ in lambdas:\n",
    "    result = cross_validate(LogisticRegression(C=lambda_), X_tr, y_tr, cv=5, return_train_score=True)\n",
    "    \n",
    "    logreg[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    logreg[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(lambdas, \"1/lambda\", logreg[\"tr_scores\"], logreg[\"va_scores\"], ylog_scale=True)\n",
    "\n",
    "# select the best lambdas and estimate the accuracy on the test set\n",
    "best_lambda = lambdas[np.argmax(logreg[\"va_scores\"])]\n",
    "print('Best lambda:', best_lambda)\n",
    "print('Test score:', \n",
    "      LogisticRegression(C=best_lambda)\n",
    "      .fit(X_tr, y_tr)\n",
    "      .score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten and normalize the X\n",
    "X_tr, y_tr = train.X.view(train.X.shape[0], -1).clone().numpy(), train.y.numpy() \n",
    "X_te, y_te = test.X.view(test.X.shape[0], -1).clone().numpy(), test.y.numpy() \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_te = scaler.transform(X_te)\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tune and compute test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-6, 6, 10) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different lambdas\n",
    "logreg = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "# tune the model on the train set \n",
    "for lambda_ in lambdas:\n",
    "    result = cross_validate(SVC(C=lambda_), X_tr, y_tr, cv=5, return_train_score=True)\n",
    "    \n",
    "    logreg[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    logreg[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(lambdas, \"1/lambda\", logreg[\"tr_scores\"], logreg[\"va_scores\"], ylog_scale=True)\n",
    "\n",
    "# select the best lambdas and estimate the accuracy on the test set\n",
    "best_lambda = lambdas[np.argmax(logreg[\"va_scores\"])]\n",
    "print('Best lambda:', best_lambda)\n",
    "print('Test score:', \n",
    "      SVC(C=best_lambda)\n",
    "      .fit(X_tr, y_tr)\n",
    "      .score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten and normalize the X\n",
    "X_tr, y_tr = train.X.view(train.X.shape[0], -1).clone().numpy(), train.y.numpy() \n",
    "X_te, y_te = test.X.view(test.X.shape[0], -1).clone().numpy(), test.y.numpy() \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_te = scaler.transform(X_te)\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tune and compute test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tols = np.logspace(-6, 1, 10) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different lambdas\n",
    "logreg = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "# tune the model on the train set \n",
    "for tol in tols:\n",
    "    result = cross_validate(LinearDiscriminantAnalysis(tol=tol), X_tr, y_tr, cv=5, return_train_score=True)\n",
    "    \n",
    "    logreg[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    logreg[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(tols, \"tol\", logreg[\"tr_scores\"], logreg[\"va_scores\"], ylog_scale=True)\n",
    "\n",
    "# select the best lambdas and estimate the accuracy on the test set\n",
    "best_tol = tols[np.argmax(logreg[\"va_scores\"])]\n",
    "print('Best tol:', best_tol)\n",
    "print('Test score:', \n",
    "      LinearDiscriminantAnalysis(tol=best_tol)\n",
    "      .fit(X_tr, y_tr)\n",
    "      .score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten, normalize the X and apply PCA to reduce dimensions (reduce curse of dimensionality effect)\n",
    "X_tr, y_tr = train.X.view(train.X.shape[0], -1).clone().numpy(), train.y.numpy() \n",
    "X_te, y_te = test.X.view(test.X.shape[0], -1).clone().numpy(), test.y.numpy() \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr_scaled = scaler.transform(X_tr)\n",
    "X_te_scaled = scaler.transform(X_te)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_tr_scaled)\n",
    "X_tr = pca.transform(X_tr)\n",
    "X_te = pca.transform(X_te)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tune and compute test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = np.arange(1, 10, 1) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different number of neighbors\n",
    "nearestNeig = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "for k in Ks:\n",
    "    result = cross_validate(\n",
    "        KNeighborsClassifier(n_neighbors=k), \n",
    "        X_tr, y_tr, cv=5, return_train_score=True)\n",
    "    \n",
    "    nearestNeig[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    nearestNeig[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(Ks, \"# Neighbors\", nearestNeig[\"tr_scores\"], nearestNeig[\"va_scores\"], ylog_scale=False)\n",
    "\n",
    "best_k = Ks[np.argmax(nearestNeig[\"va_scores\"])]\n",
    "print('Best k:', best_k)\n",
    "print('Test score:', \n",
    "      KNeighborsClassifier(n_neighbors=k)\n",
    "      .fit(X_tr, y_tr)\n",
    "      .score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN: 2D convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a channel to store the pixels image (so to apply the 2D convolutional layer)\n",
    "X_tr, y_tr = Variable(train.X.clone().unsqueeze(1)), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone().unsqueeze(1)), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network with two convolutional layers\n",
    "class CNN1(Module, modelWrapper):\n",
    "    def __init__(self, nb_hidden):\n",
    "        self.nb_hidden = nb_hidden\n",
    "        super(CNN1, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=(3, 7), padding=(1, 3)) \n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.conv3 = Conv2d(64, 32, kernel_size=5, padding=2)\n",
    "#         self.conv4 = Conv2d(32, 16, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.fc1 = Linear(384, nb_hidden)\n",
    "        self.fc2 = Linear(nb_hidden, 2)\n",
    "        \n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (1, 28, 50)\n",
    "        x = F.leaky_relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # (32, 14, 25)\n",
    "        x = F.leaky_relu(F.max_pool2d(self.conv2(x), (2, 5)))\n",
    "        # (64, 7, 5)\n",
    "        x = F.leaky_relu(F.max_pool2d(self.conv3(x), 2, padding=(1, 1)))\n",
    "        # (32, 4, 3)\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x.view(-1, 384)))  \n",
    "        # (1, nb_hidden)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__(self.nb_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN1(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(train.X.variable).shape, train.X.variable.shape\n",
    "# model.score(train.X.variable, train.y.variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(Module, modelWrapper):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        \n",
    "        self.conv1_1 = Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = Conv2d(32, 32, kernel_size=3, padding=1) \n",
    "        self.conv1_3 = Conv2d(32, 32, kernel_size=3, padding=1) \n",
    "        \n",
    "        self.conv2_1 = Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_3 = Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv3_1 = Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = Linear(1536, 200)\n",
    "        self.fc2 = Linear(200, 2)\n",
    "        \n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (1, 28, 50)\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv1_3(x), 2))\n",
    "        # (32, 14, 25)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_3(x), (2, 5)))\n",
    "        # (64, 7, 5)\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv3_3(x), 2, padding=(1, 1)))\n",
    "        # (128, 4, 3)\n",
    "        \n",
    "        x = F.relu(self.fc1(x.view(-1, 1536)))  \n",
    "        # (1, nb_hidden)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__(self.nb_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN2()\n",
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual network with 2D convolutional layers + batch normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a channel to store the pixels image (so to apply the 2D convolutional layer)\n",
    "X_tr, y_tr = Variable(train.X.clone().unsqueeze(1)), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone().unsqueeze(1)), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resudial network\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "    \n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module, modelWrapper):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = int((depth - 4) / 6)\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(1, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        #self.nChannels = nChannels[3]\n",
    "        self.fc = nn.Linear(1152, num_classes)\n",
    "        self.nChannels = 1152\n",
    "        \n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "#                 m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.weight.data.fill_(1)\n",
    "#                 m.bias.data.zero_()\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        out = self.conv1(x)\n",
    "        #print(out.size())\n",
    "        out = self.block1(out)\n",
    "        #print(out.size())\n",
    "        out = self.block2(out)\n",
    "        #print(out.size())\n",
    "        out = self.block3(out)\n",
    "        #print(out.size())\n",
    "        out = self.relu(self.bn1(out))\n",
    "        #print(out.size())\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        #print(out.size())\n",
    "        out = self.fc(out.view(-1, self.nChannels))\n",
    "        #print(out.size())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(depth=16, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1D convolution +  dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network with two convolutional layers\n",
    "class CNN_1D_Dropout(Module, modelWrapper):\n",
    "    def __init__(self, nb_hidden=50):\n",
    "        self.nb_hidden = nb_hidden\n",
    "        super(CNN_1D_Dropout, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv1d(28, 64, kernel_size=5, padding=2) \n",
    "        self.conv2 = Conv1d(64, 64, kernel_size=5, padding=2)\n",
    "        self.conv3 = Conv1d(64, 32, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.fc1 = Linear(448, nb_hidden)\n",
    "        self.fc2 = Linear(nb_hidden, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (28, 50)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        # (64, 50)\n",
    "        x = F.relu(F.max_pool1d(self.conv2(x), 2, padding=1))\n",
    "        x = self.dropout(x)\n",
    "        # (64, 26)\n",
    "        x = F.relu(F.max_pool1d(self.conv3(x), 2, padding=1))\n",
    "        x = self.dropout(x)\n",
    "        # (32, 14)\n",
    "        \n",
    "        x = F.relu(self.fc1(x.view(-1, 448)))  \n",
    "        # (1, nb_hidden)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__(self.nb_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_1D_Dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D convolution + Batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D_BatchNorm(Module, modelWrapper):\n",
    "    def __init__(self, nb_hidden=50, activation=F.relu):\n",
    "        torch.manual_seed(0) \n",
    "        super(CNN_1D_BatchNorm, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.nb_hidden = nb_hidden\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(28)\n",
    "        self.conv1 = Conv1d(28, 64, kernel_size=5, padding=2) \n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = Conv1d(64, 64, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = Conv1d(64, 32, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.fc1 = Linear(448, nb_hidden)\n",
    "        self.fc2 = Linear(nb_hidden, 2)\n",
    "        \n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (28, 50)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(self.conv1(x))\n",
    "\n",
    "        # (64, 50)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation(F.max_pool1d(self.conv2(x), 2, padding=1))\n",
    "\n",
    "        # (64, 26)\n",
    "        x = self.bn3(x)\n",
    "        x = self.activation(F.max_pool1d(self.conv3(x), 2, padding=1))\n",
    "\n",
    "        # (32, 14)\n",
    "#         x = self.bn4(x)\n",
    "        x = x.view(-1, 448)\n",
    "        x = self.activation(self.fc1(x))  \n",
    "    \n",
    "        # (1, nb_hidden)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_extraction(self):\n",
    "        # (28, 50)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(self.conv1(x))\n",
    "\n",
    "        # (64, 50)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation(F.max_pool1d(self.conv2(x), 2, padding=1))\n",
    "\n",
    "        # (64, 26)\n",
    "        x = self.bn3(x)\n",
    "        x = self.activation(F.max_pool1d(self.conv3(x), 2, padding=1))\n",
    "\n",
    "        # (32, 14)\n",
    "#         x = self.bn4(x)\n",
    "        return x.view(-1, 448)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__(self.nb_hidden, self.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_1D_BatchNorm(nb_hidden=50, activation=F.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=25, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D convolution + Batch norm (bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D_BatchNorm_Big(Module, modelWrapper):\n",
    "    def __init__(self, nb_hidden=50):\n",
    "        self.nb_hidden = nb_hidden\n",
    "        super(CNN_1D_BatchNorm_Big, self).__init__()\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(28)\n",
    "        self.conv1 = Conv1d(28, 64, kernel_size=5, padding=2) \n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = Conv1d(64, 64, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.conv4 = Conv1d(128, 64, kernel_size=5, padding=2) \n",
    "        \n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.conv5 = Conv1d(64, 32, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.bn6 = nn.BatchNorm1d(32)\n",
    "        self.conv6 = Conv1d(32, 16, kernel_size=5, padding=2)\n",
    "        \n",
    "#         self.bn1 = nn.BatchNorm1d(448)\n",
    "        self.fc1 = Linear(224, nb_hidden)\n",
    "        self.fc2 = Linear(nb_hidden, 2)\n",
    "        \n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (28, 50)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        # (64, 50)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # (64, 50)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(F.max_pool1d(self.conv3(x), 2, padding=1))\n",
    "\n",
    "        # (128, 26)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(F.max_pool1d(self.conv4(x), 2, padding=1))\n",
    "        \n",
    "        # (64, 14)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        \n",
    "        # (16, 14)\n",
    "        x = self.bn6(x)\n",
    "        x = F.relu(self.conv6(x))\n",
    "        \n",
    "        x = F.relu(self.fc1(x.view(-1, 224)))  \n",
    "        # (1, nb_hidden)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__(self.nb_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_1D_BatchNorm_Big()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D convolution + Batch norm + Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual_block(Module):\n",
    "    def __init__(self, activation=F.relu):\n",
    "        super(residual_block, self).__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv1 = Conv1d(32, 64, kernel_size=3, padding=1) \n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = Conv1d(64, 32, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (32, 50)\n",
    "        out = self.bn1(x)\n",
    "        out = self.activation(self.conv1(out))\n",
    "\n",
    "        # (64, 50)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(self.conv2(out))\n",
    "\n",
    "        # (32, 50)\n",
    "        out = self.bn3(out)\n",
    "        out = self.activation(self.conv3(out))\n",
    "        \n",
    "        return out+x\n",
    "\n",
    "class aggregated_residual_blocks(Module):\n",
    "    def __init__(self, n_residual_blocks=2, activation=F.relu):\n",
    "        super(aggregated_residual_blocks, self).__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "        for i in range(n_residual_blocks):\n",
    "            self.residual_blocks.append(residual_block())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        \n",
    "        for block in self.residual_blocks:\n",
    "            out.append(block(x))\n",
    "            \n",
    "        return sum(out)+x\n",
    "#         return self.residual_blocks[0](x) + self.residual_blocks[1](x) + self.residual_blocks[2](x) + x\n",
    "    \n",
    "class CNN_1D_BatchNorm_Residual(Module, modelWrapper):\n",
    "    def __init__(self, nb_hidden=50, n_aggregated_residual_blocks=2, n_residual_blocks=2, activation=F.relu):\n",
    "        # n_aggregated_residual_blocks: number of aggregated residual blocks (aggregated_residual_blocks)\n",
    "        # n_residual_blocks: number of residual blocks per aggregated residual block\n",
    "        \n",
    "        super(CNN_1D_BatchNorm_Residual, self).__init__()\n",
    "        self.nb_hidden = nb_hidden\n",
    "        self.n_residual_blocks=n_residual_blocks\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(28)\n",
    "        self.conv1 = Conv1d(28, 32, kernel_size=5, padding=2) \n",
    "        \n",
    "        self.agg_residual_blocks = nn.ModuleList()\n",
    "        for i in range(n_aggregated_residual_blocks):\n",
    "            self.agg_residual_blocks.append(aggregated_residual_blocks(n_residual_blocks))\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = Conv1d(32, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = Conv1d(32, 16, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = Linear(208, nb_hidden)\n",
    "        self.fc2 = Linear(nb_hidden, 2)\n",
    "        \n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (28, 50) -> (32, 50)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(self.conv1(x))\n",
    "\n",
    "        # (32, 50) -> (32, 50)\n",
    "        for block in self.agg_residual_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # (32, 50) -> (32, 26)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation(F.max_pool1d(self.conv2(x), 2, padding=1))\n",
    "        \n",
    "        # (32, 26) -> (16, 13)\n",
    "        x = self.bn3(x)\n",
    "        x = self.activation(F.max_pool1d(self.conv3(x), 2))\n",
    "\n",
    "        # (16, 13) -> (208)\n",
    "        x = self.activation(self.fc1(x.view(-1, 208))) \n",
    "        \n",
    "        # (1, nb_hidden)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__(self.nb_hidden, self.n_residual_blocks, self.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN_1D_BatchNorm_Residual(n_aggregated_residual_blocks=2, n_residual_blocks=5, activation=F.relu)\n",
    "model = CNN_1D_BatchNorm_Residual(n_aggregated_residual_blocks=2, n_residual_blocks=2, activation=F.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D conv horizontal and 1D conv vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D(Module):\n",
    "    def __init__(self, in_channels, activation=F.relu):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.activation = activation\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(in_channels)\n",
    "        self.conv1 = Conv1d(in_channels, 64, kernel_size=7, padding=3) \n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = Conv1d(64, 64, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = Conv1d(64, in_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (in_channels, 50)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(self.conv1(x))\n",
    "\n",
    "        # (64, 50)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation(self.conv2(x))\n",
    "\n",
    "        # (64, in_channels)\n",
    "        x = self.bn3(x)\n",
    "        x = self.activation(self.conv3(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "class conv2D(Module):\n",
    "    def __init__(self, activation=F.relu):\n",
    "        super(conv2D, self).__init__()\n",
    "        self.activation = activation\n",
    "                \n",
    "        self.bn1 = nn.BatchNorm1d(1)\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=(3, 7), padding=(1, 3))\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = Conv2d(32, 32, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = Conv2d(32, 16, kernel_size=5, padding=2)\n",
    "#         self.conv4 = Conv2d(32, 16, kernel_size=5, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (1, 28, 50) -> (32, 14, 26)\n",
    "        x = self.activation(F.max_pool2d(self.conv1(self.bn1(x)), 2, padding=(0, 1)))\n",
    "\n",
    "        # (32, 14, 26) -> (32, 8, 14)\n",
    "        x = self.activation(F.max_pool2d(self.conv2(self.bn2(x)), (2, 2), padding=(1, 1)))\n",
    "\n",
    "        # (32, 8, 14) -> (16, 4, 7)\n",
    "        x = self.activation(F.max_pool2d(self.conv3(self.bn3(x)), 2))\n",
    "\n",
    "        return x\n",
    "        \n",
    "class horiz_vert_1D(Module, modelWrapper):\n",
    "    def __init__(self, activation=F.relu, nb_hidden=50):\n",
    "        super(horiz_vert_1D, self).__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.horiz = CNN_1D(28, activation=activation)\n",
    "        self.vert = CNN_1D(50, activation=activation)\n",
    "        \n",
    "        self.conv2D = conv2D()\n",
    "        self.conv2D_horiz = conv2D()\n",
    "        self.conv2D_vert = conv2D()\n",
    "        \n",
    "        self.fc1 = Linear(448, nb_hidden)\n",
    "        self.fc2 = Linear(nb_hidden, 2)\n",
    "        \n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.conv2D_horiz(self.horiz(x).unsqueeze(1))\n",
    "        out2 = self.conv2D_vert(self.vert(x.transpose(1, 2)).transpose(1, 2).unsqueeze(1))\n",
    "        out3 = self.conv2D(x.unsqueeze(1))\n",
    "        \n",
    "        out = out1 + out2 + out3\n",
    "        \n",
    "        out = self.activation(self.fc1(out.view(-1, 448))) \n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def reset(self):\n",
    "        self.__init__(self.nb_hidden, self.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = horiz_vert_1D(nb_hidden=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
