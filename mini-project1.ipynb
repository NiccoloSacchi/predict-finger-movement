{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1>Mini-project1</h1>\n",
    "    <h2>Predict the laterality of upcoming finger movements</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import dlc_bci as bci\n",
    "from types import SimpleNamespace \n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# baselines\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from helpers import *\n",
    "from modelWrapper import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download/load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n",
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "one_khz=False\n",
    "\n",
    "train = SimpleNamespace()\n",
    "train.X, train.y = bci.load(root='./data_bci', one_khz=one_khz)\n",
    "print(str(type(train.X)), train.X.size())\n",
    "print(str(type(train.y)), train.y.size())\n",
    "\n",
    "test = SimpleNamespace()\n",
    "test.X, test.y = bci.load(root='./data_bci', train=False, one_khz=one_khz)\n",
    "print(str(type(test.X)), test.X.size())\n",
    "print(str(type(test.y)), test.y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-45e1be675b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tr' is not defined"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "plt.imshow(X_tr[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((316, 1400), (316,), (100, 1400), (100,))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten the X (no standardization for random forest) \n",
    "X_tr, y_tr = train.X.view(train.X.shape[0], -1).clone().numpy(), train.y.numpy() \n",
    "X_te, y_te = test.X.view(test.X.shape[0], -1).clone().numpy(), test.y.numpy() \n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tune and compute test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best depth: 5\n",
      "Test score: 0.58\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAAFACAYAAAAoIqKDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkyQV4/8DBzDEDKyuonMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f085eb60dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "depths = np.arange(5, 100, 10) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different depths\n",
    "randForest = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "for depth in depths:\n",
    "    result = cross_validate(\n",
    "        RandomForestClassifier(n_estimators=100, max_depth=depth, n_jobs=-1, random_state=1), \n",
    "        X_tr, y_tr, cv=5, return_train_score=True)\n",
    "    \n",
    "    randForest[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    randForest[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(depths, \"depth\", randForest[\"tr_scores\"], randForest[\"va_scores\"], ylog_scale=False)\n",
    "\n",
    "best_depth = depths[np.argmax(randForest[\"va_scores\"])]\n",
    "print('Best depth:', best_depth)\n",
    "print('Test score:',\n",
    "      RandomForestClassifier(n_estimators=100, max_depth=depth, n_jobs=-1, random_state=1)\n",
    "      .fit(X_tr, y_tr)\n",
    "      .score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((316, 1400), (316,), (100, 1400), (100,))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten and normalize the X\n",
    "X_tr, y_tr = train.X.view(train.X.shape[0], -1).clone().numpy(), train.y.numpy() \n",
    "X_te, y_te = test.X.view(test.X.shape[0], -1).clone().numpy(), test.y.numpy() \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_te = scaler.transform(X_te)\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tune and compute test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.215443469003\n",
      "Test score: 0.7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAAFECAYAAACzs+CVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f085ea2f128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lambdas = np.logspace(-6, 6, 10) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different lambdas\n",
    "logreg = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "# tune the model on the train set \n",
    "for lambda_ in lambdas:\n",
    "    result = cross_validate(LogisticRegression(C=lambda_), X_tr, y_tr, cv=5, return_train_score=True)\n",
    "    \n",
    "    logreg[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    logreg[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(lambdas, \"1/lambda\", logreg[\"tr_scores\"], logreg[\"va_scores\"], ylog_scale=True)\n",
    "\n",
    "# select the best lambdas and estimate the accuracy on the test set\n",
    "best_lambda = lambdas[np.argmax(logreg[\"va_scores\"])]\n",
    "print('Best lambda:', best_lambda)\n",
    "print('Test score:', \n",
    "      LogisticRegression(C=best_lambda)\n",
    "      .fit(X_tr, y_tr)\n",
    "      .score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten and normalize the X\n",
    "X_tr, y_tr = train.X.view(train.X.shape[0], -1).clone().numpy(), train.y.numpy() \n",
    "X_te, y_te = test.X.view(test.X.shape[0], -1).clone().numpy(), test.y.numpy() \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_te = scaler.transform(X_te)\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tune and compute test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-6, 6, 10) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different lambdas\n",
    "logreg = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "# tune the model on the train set \n",
    "for lambda_ in lambdas:\n",
    "    result = cross_validate(SVC(C=lambda_), X_tr, y_tr, cv=5, return_train_score=True)\n",
    "    \n",
    "    logreg[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    logreg[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(lambdas, \"1/lambda\", logreg[\"tr_scores\"], logreg[\"va_scores\"], ylog_scale=True)\n",
    "\n",
    "# select the best lambdas and estimate the accuracy on the test set\n",
    "best_lambda = lambdas[np.argmax(logreg[\"va_scores\"])]\n",
    "print('Best lambda:', best_lambda)\n",
    "print('Test score:', \n",
    "      SVC(C=best_lambda)\n",
    "      .fit(X_tr, y_tr)\n",
    "      .score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten and normalize the X\n",
    "X_tr, y_tr = train.X.view(train.X.shape[0], -1).clone().numpy(), train.y.numpy() \n",
    "X_te, y_te = test.X.view(test.X.shape[0], -1).clone().numpy(), test.y.numpy() \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_te = scaler.transform(X_te)\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tune and compute test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tols = np.logspace(-6, 1, 10) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different lambdas\n",
    "logreg = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "# tune the model on the train set \n",
    "for tol in tols:\n",
    "    result = cross_validate(LinearDiscriminantAnalysis(tol=tol), X_tr, y_tr, cv=5, return_train_score=True)\n",
    "    \n",
    "    logreg[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    logreg[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(tols, \"tol\", logreg[\"tr_scores\"], logreg[\"va_scores\"], ylog_scale=True)\n",
    "\n",
    "# select the best lambdas and estimate the accuracy on the test set\n",
    "best_tol = tols[np.argmax(logreg[\"va_scores\"])]\n",
    "print('Best tol:', best_tol)\n",
    "print('Test score:', \n",
    "      LinearDiscriminantAnalysis(tol=best_tol)\n",
    "      .fit(X_tr, y_tr)\n",
    "      .score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten, normalize the X and apply PCA to reduce dimensions (reduce curse of dimensionality effect)\n",
    "X_tr, y_tr = train.X.view(train.X.shape[0], -1).clone().numpy(), train.y.numpy() \n",
    "X_te, y_te = test.X.view(test.X.shape[0], -1).clone().numpy(), test.y.numpy() \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr_scaled = scaler.transform(X_tr)\n",
    "X_te_scaled = scaler.transform(X_te)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_tr_scaled)\n",
    "X_tr = pca.transform(X_tr)\n",
    "X_te = pca.transform(X_te)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tune and compute test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = np.arange(1, 10, 1) # grid search on a parameter of the model\n",
    "\n",
    "# here we store all the scores obtained with the different number of neighbors\n",
    "nearestNeig = {\n",
    "    \"tr_scores\": [],\n",
    "    \"va_scores\": []\n",
    "}\n",
    "\n",
    "for k in Ks:\n",
    "    result = cross_validate(\n",
    "        KNeighborsClassifier(n_neighbors=k), \n",
    "        X_tr, y_tr, cv=5, return_train_score=True)\n",
    "    \n",
    "    nearestNeig[\"tr_scores\"].append(np.mean(result[\"train_score\"]))\n",
    "    nearestNeig[\"va_scores\"].append(np.mean(result[\"test_score\"]))\n",
    "    \n",
    "plot_scores(Ks, \"# Neighbors\", nearestNeig[\"tr_scores\"], nearestNeig[\"va_scores\"], ylog_scale=False)\n",
    "\n",
    "best_k = Ks[np.argmax(nearestNeig[\"va_scores\"])]\n",
    "print('Best k:', best_k)\n",
    "print('Test score:', \n",
    "      KNeighborsClassifier(n_neighbors=k)\n",
    "      .fit(X_tr, y_tr)\n",
    "      .score(X_te, y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN: 2D convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([316, 1, 28, 50]),\n",
       " torch.Size([316]),\n",
       " torch.Size([100, 1, 28, 50]),\n",
       " torch.Size([100]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a channel to store the pixels image (so to apply the 2D convolutional layer)\n",
    "X_tr, y_tr = Variable(train.X.clone().unsqueeze(1)), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone().unsqueeze(1)), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2D(modelWrapper):\n",
    "    def __init__(self, nb_hidden=50, activation = nn.ReLU):\n",
    "        super(CNN2D, self).__init__()        \n",
    "        \n",
    "        self.activation = activation\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 7), padding=(1, 3)),\n",
    "            nn.MaxPool2d(2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d((2, 5)),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=5, padding=2),\n",
    "            nn.MaxPool2d(2, padding=(1, 1)),\n",
    "            self.activation(),\n",
    "        )\n",
    "        \n",
    "        self.num_features = 384\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features, nb_hidden),\n",
    "            self.activation(),\n",
    "            nn.Linear(nb_hidden, 2)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 17.735598146915436. Train accuracy 49.68%. Test accuracy 51.00%\n",
      "Epoch 1: Train loss: 11.17670351266861. Train accuracy 50.00%. Test accuracy 49.00%\n",
      "Epoch 2: Train loss: 11.066096782684326. Train accuracy 59.81%. Test accuracy 49.00%\n",
      "Epoch 3: Train loss: 11.02427351474762. Train accuracy 57.59%. Test accuracy 45.00%\n",
      "Epoch 4: Train loss: 10.97516816854477. Train accuracy 64.24%. Test accuracy 54.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN2D(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 7), stride=(1, 1), padding=(1, 3))\n",
       "    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=(2, 5), stride=(2, 5), dilation=(1, 1), ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss(\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1D convolution +  dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([316, 28, 50]),\n",
       " torch.Size([316]),\n",
       " torch.Size([100, 28, 50]),\n",
       " torch.Size([100]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network with two convolutional layers\n",
    "class CNN_1D_Dropout(modelWrapper):\n",
    "    def __init__(self, nb_hidden=50, activation=nn.ReLU):\n",
    "        self.nb_hidden = nb_hidden\n",
    "        super(CNN_1D_Dropout, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(28, 64, kernel_size=5, padding=2),\n",
    "            self.activation(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.MaxPool1d(2, padding=1),\n",
    "            self.activation(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv1d(64, 32, kernel_size=5, padding=2),\n",
    "            nn.MaxPool1d(2, padding=1),\n",
    "            self.activation(),\n",
    "        )\n",
    "        \n",
    "        self.num_features = 448\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features, nb_hidden),\n",
    "            self.activation(),\n",
    "            nn.Linear(nb_hidden, 2)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_1D_Dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 12.603899657726288. Train accuracy 53.80%. Test accuracy 50.00%\n",
      "Epoch 1: Train loss: 11.29260778427124. Train accuracy 50.63%. Test accuracy 54.00%\n",
      "Epoch 2: Train loss: 10.887699127197266. Train accuracy 60.76%. Test accuracy 58.00%\n",
      "Epoch 3: Train loss: 10.676740527153015. Train accuracy 64.56%. Test accuracy 52.00%\n",
      "Epoch 4: Train loss: 10.483382821083069. Train accuracy 68.35%. Test accuracy 55.00%\n",
      "Epoch 5: Train loss: 9.79628637433052. Train accuracy 75.95%. Test accuracy 65.00%\n",
      "Epoch 6: Train loss: 8.87550476193428. Train accuracy 74.37%. Test accuracy 67.00%\n",
      "Epoch 7: Train loss: 8.62815609574318. Train accuracy 71.52%. Test accuracy 65.00%\n",
      "Epoch 8: Train loss: 8.24027106165886. Train accuracy 82.28%. Test accuracy 76.00%\n",
      "Epoch 9: Train loss: 6.475054711103439. Train accuracy 68.04%. Test accuracy 67.00%\n",
      "Epoch 10: Train loss: 7.116798236966133. Train accuracy 83.86%. Test accuracy 74.00%\n",
      "Epoch 11: Train loss: 7.548905938863754. Train accuracy 78.16%. Test accuracy 69.00%\n",
      "Epoch 12: Train loss: 7.754688322544098. Train accuracy 84.49%. Test accuracy 69.00%\n",
      "Epoch 13: Train loss: 6.324954003095627. Train accuracy 87.97%. Test accuracy 73.00%\n",
      "Epoch 14: Train loss: 6.308505564928055. Train accuracy 87.66%. Test accuracy 73.00%\n",
      "Epoch 15: Train loss: 5.6526942402124405. Train accuracy 90.19%. Test accuracy 73.00%\n",
      "Epoch 16: Train loss: 5.550312593579292. Train accuracy 87.03%. Test accuracy 70.00%\n",
      "Epoch 17: Train loss: 6.151773646473885. Train accuracy 86.39%. Test accuracy 69.00%\n",
      "Epoch 18: Train loss: 6.2411249577999115. Train accuracy 88.61%. Test accuracy 73.00%\n",
      "Epoch 19: Train loss: 4.5198967307806015. Train accuracy 80.38%. Test accuracy 76.00%\n",
      "Epoch 20: Train loss: 4.901383593678474. Train accuracy 78.48%. Test accuracy 68.00%\n",
      "Epoch 21: Train loss: 6.300886750221252. Train accuracy 90.82%. Test accuracy 75.00%\n",
      "Epoch 22: Train loss: 5.294185906648636. Train accuracy 88.29%. Test accuracy 71.00%\n",
      "Epoch 23: Train loss: 4.671877898275852. Train accuracy 90.51%. Test accuracy 70.00%\n",
      "Epoch 24: Train loss: 5.4074776619672775. Train accuracy 81.33%. Test accuracy 69.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_1D_Dropout(\n",
       "  (features): Sequential(\n",
       "    (0): Conv1d(28, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1)\n",
       "    (3): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (4): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.1)\n",
       "    (7): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (8): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (9): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss(\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D convolution + Batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([316, 28, 50]),\n",
       " torch.Size([316]),\n",
       " torch.Size([100, 28, 50]),\n",
       " torch.Size([100]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D_BatchNorm(modelWrapper):\n",
    "    def __init__(self, nb_hidden=50, activation=nn.ReLU):\n",
    "        torch.manual_seed(0) \n",
    "        super(CNN_1D_BatchNorm, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.BatchNorm1d(28),\n",
    "            nn.Conv1d(28, 32, kernel_size=5, padding=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 32, kernel_size=5, padding=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 32, kernel_size=5, padding=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 32, kernel_size=5, padding=2),\n",
    "            nn.MaxPool1d(2, padding=1),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 32, kernel_size=5, padding=2),\n",
    "            nn.MaxPool1d(2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(32),\n",
    "        )\n",
    "        \n",
    "        self.num_features = 32*13\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features, nb_hidden),\n",
    "            self.activation(),\n",
    "            nn.Linear(nb_hidden, 2)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "#         self.optimizer = Adam(self.parameters(), lr=0.0001, weight_decay=1e-1)\n",
    "        self.optimizer = optim.Adamax(self.parameters(), lr=0.001, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_1D_BatchNorm(nb_hidden=250, activation=nn.PReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 4.897167801856995. Train accuracy 50.32%. Test accuracy 49.00%\n",
      "Epoch 1: Train loss: 4.475085616111755. Train accuracy 50.32%. Test accuracy 49.00%\n",
      "Epoch 2: Train loss: 4.137418061494827. Train accuracy 50.32%. Test accuracy 49.00%\n",
      "Epoch 3: Train loss: 3.783247262239456. Train accuracy 50.32%. Test accuracy 49.00%\n",
      "Epoch 4: Train loss: 3.502762198448181. Train accuracy 70.25%. Test accuracy 57.00%\n",
      "Epoch 5: Train loss: 3.1077880859375. Train accuracy 81.96%. Test accuracy 67.00%\n",
      "Epoch 6: Train loss: 2.7011975944042206. Train accuracy 83.23%. Test accuracy 75.00%\n",
      "Epoch 7: Train loss: 2.505679413676262. Train accuracy 85.13%. Test accuracy 79.00%\n",
      "Epoch 8: Train loss: 2.2265518605709076. Train accuracy 87.34%. Test accuracy 74.00%\n",
      "Epoch 9: Train loss: 2.0717523247003555. Train accuracy 85.13%. Test accuracy 78.00%\n",
      "Epoch 10: Train loss: 1.9938741326332092. Train accuracy 90.19%. Test accuracy 77.00%\n",
      "Epoch 11: Train loss: 1.6451536566019058. Train accuracy 90.51%. Test accuracy 79.00%\n",
      "Epoch 12: Train loss: 1.5760670602321625. Train accuracy 91.77%. Test accuracy 78.00%\n",
      "Epoch 13: Train loss: 1.482830986380577. Train accuracy 94.62%. Test accuracy 78.00%\n",
      "Epoch 14: Train loss: 1.3193134292960167. Train accuracy 94.94%. Test accuracy 80.00%\n",
      "Epoch 15: Train loss: 1.1971296072006226. Train accuracy 96.20%. Test accuracy 80.00%\n",
      "Epoch 16: Train loss: 1.1867113672196865. Train accuracy 94.62%. Test accuracy 80.00%\n",
      "Epoch 17: Train loss: 1.1062328815460205. Train accuracy 97.15%. Test accuracy 78.00%\n",
      "Epoch 18: Train loss: 0.8920763414353132. Train accuracy 95.57%. Test accuracy 79.00%\n",
      "Epoch 19: Train loss: 0.9233905449509621. Train accuracy 97.78%. Test accuracy 78.00%\n",
      "Epoch 20: Train loss: 0.896748460829258. Train accuracy 97.47%. Test accuracy 80.00%\n",
      "Epoch 21: Train loss: 0.7080847751349211. Train accuracy 97.15%. Test accuracy 77.00%\n",
      "Epoch 22: Train loss: 0.7636555805802345. Train accuracy 91.14%. Test accuracy 77.00%\n",
      "Epoch 23: Train loss: 0.8997793048620224. Train accuracy 98.42%. Test accuracy 79.00%\n",
      "Epoch 24: Train loss: 0.5891149966046214. Train accuracy 97.78%. Test accuracy 77.00%\n",
      "Epoch 25: Train loss: 0.6093953819945455. Train accuracy 98.73%. Test accuracy 81.00%\n",
      "Epoch 26: Train loss: 0.5373403541743755. Train accuracy 97.47%. Test accuracy 83.00%\n",
      "Epoch 27: Train loss: 0.5547733921557665. Train accuracy 98.73%. Test accuracy 82.00%\n",
      "Epoch 28: Train loss: 0.4781511314213276. Train accuracy 96.84%. Test accuracy 78.00%\n",
      "Epoch 29: Train loss: 0.49685249431058764. Train accuracy 99.68%. Test accuracy 85.00%\n",
      "Epoch 30: Train loss: 0.34602765925228596. Train accuracy 99.68%. Test accuracy 81.00%\n",
      "Epoch 31: Train loss: 0.3068028371781111. Train accuracy 100.00%. Test accuracy 83.00%\n",
      "Epoch 32: Train loss: 0.45502325147390366. Train accuracy 97.78%. Test accuracy 79.00%\n",
      "Epoch 33: Train loss: 0.31358773401007056. Train accuracy 98.10%. Test accuracy 83.00%\n",
      "Epoch 34: Train loss: 0.3022581348195672. Train accuracy 99.05%. Test accuracy 79.00%\n",
      "Epoch 35: Train loss: 0.24809318874031305. Train accuracy 99.05%. Test accuracy 79.00%\n",
      "Epoch 36: Train loss: 0.3051476310938597. Train accuracy 99.05%. Test accuracy 83.00%\n",
      "Epoch 37: Train loss: 0.26333841728046536. Train accuracy 99.37%. Test accuracy 80.00%\n",
      "Epoch 38: Train loss: 0.25916144251823425. Train accuracy 99.68%. Test accuracy 80.00%\n",
      "Epoch 39: Train loss: 0.24369812151417136. Train accuracy 99.68%. Test accuracy 79.00%\n",
      "Epoch 40: Train loss: 0.26410072948783636. Train accuracy 99.05%. Test accuracy 80.00%\n",
      "Epoch 41: Train loss: 0.2143661780282855. Train accuracy 99.68%. Test accuracy 79.00%\n",
      "Epoch 42: Train loss: 0.24833669047802687. Train accuracy 100.00%. Test accuracy 84.00%\n",
      "Epoch 43: Train loss: 0.2370238082949072. Train accuracy 100.00%. Test accuracy 83.00%\n",
      "Epoch 44: Train loss: 0.23279422707855701. Train accuracy 100.00%. Test accuracy 84.00%\n",
      "Epoch 45: Train loss: 0.22612390643917024. Train accuracy 99.37%. Test accuracy 85.00%\n",
      "Epoch 46: Train loss: 0.20949165103957057. Train accuracy 99.37%. Test accuracy 84.00%\n",
      "Epoch 47: Train loss: 0.20446325046941638. Train accuracy 99.68%. Test accuracy 83.00%\n",
      "Epoch 48: Train loss: 0.1322239653673023. Train accuracy 100.00%. Test accuracy 82.00%\n",
      "Epoch 49: Train loss: 0.12034587957896292. Train accuracy 100.00%. Test accuracy 80.00%\n",
      "Epoch 50: Train loss: 0.18290162831544876. Train accuracy 100.00%. Test accuracy 78.00%\n",
      "Epoch 51: Train loss: 0.10780258011072874. Train accuracy 99.68%. Test accuracy 81.00%\n",
      "Epoch 52: Train loss: 0.117401730036363. Train accuracy 99.68%. Test accuracy 79.00%\n",
      "Epoch 53: Train loss: 0.10932457633316517. Train accuracy 100.00%. Test accuracy 81.00%\n",
      "Epoch 54: Train loss: 0.16004752134904265. Train accuracy 99.37%. Test accuracy 78.00%\n",
      "Epoch 55: Train loss: 0.10672747110947967. Train accuracy 99.37%. Test accuracy 77.00%\n",
      "Epoch 56: Train loss: 0.13494909333530813. Train accuracy 99.37%. Test accuracy 80.00%\n",
      "Epoch 57: Train loss: 0.16228151065297425. Train accuracy 98.73%. Test accuracy 80.00%\n",
      "Epoch 58: Train loss: 0.12230051681399345. Train accuracy 99.68%. Test accuracy 78.00%\n",
      "Epoch 59: Train loss: 0.09538921574130654. Train accuracy 99.68%. Test accuracy 80.00%\n",
      "Epoch 60: Train loss: 0.0847126767039299. Train accuracy 100.00%. Test accuracy 83.00%\n",
      "Epoch 61: Train loss: 0.08406103169545531. Train accuracy 100.00%. Test accuracy 79.00%\n",
      "Epoch 62: Train loss: 0.11522417573723942. Train accuracy 99.68%. Test accuracy 80.00%\n",
      "Epoch 63: Train loss: 0.10214176075533032. Train accuracy 100.00%. Test accuracy 79.00%\n",
      "Epoch 64: Train loss: 0.09514443180523813. Train accuracy 100.00%. Test accuracy 83.00%\n",
      "Epoch 65: Train loss: 0.08333482872694731. Train accuracy 100.00%. Test accuracy 82.00%\n",
      "Epoch 66: Train loss: 0.10274015739560127. Train accuracy 99.37%. Test accuracy 80.00%\n",
      "Epoch 67: Train loss: 0.09833922376856208. Train accuracy 100.00%. Test accuracy 79.00%\n",
      "Epoch 68: Train loss: 0.09567768266424537. Train accuracy 99.37%. Test accuracy 78.00%\n",
      "Epoch 69: Train loss: 0.10808788170106709. Train accuracy 100.00%. Test accuracy 83.00%\n",
      "Epoch 70: Train loss: 0.10631811106577516. Train accuracy 99.37%. Test accuracy 81.00%\n",
      "Epoch 71: Train loss: 0.09890448953956366. Train accuracy 99.68%. Test accuracy 83.00%\n",
      "Epoch 72: Train loss: 0.1245708572678268. Train accuracy 99.05%. Test accuracy 82.00%\n",
      "Epoch 73: Train loss: 0.09828269737772644. Train accuracy 99.05%. Test accuracy 76.00%\n",
      "Epoch 74: Train loss: 0.126581319142133. Train accuracy 99.37%. Test accuracy 80.00%\n",
      "Epoch 75: Train loss: 0.17933360254392028. Train accuracy 99.37%. Test accuracy 82.00%\n",
      "Epoch 76: Train loss: 0.10462252097204328. Train accuracy 99.37%. Test accuracy 83.00%\n",
      "Epoch 77: Train loss: 0.08506361895706505. Train accuracy 99.05%. Test accuracy 82.00%\n",
      "Epoch 78: Train loss: 0.16924566635861993. Train accuracy 98.42%. Test accuracy 82.00%\n",
      "Epoch 79: Train loss: 0.18084664223715663. Train accuracy 99.68%. Test accuracy 80.00%\n",
      "Epoch 80: Train loss: 0.10272482456639409. Train accuracy 98.73%. Test accuracy 82.00%\n",
      "Epoch 81: Train loss: 0.10622964962385595. Train accuracy 100.00%. Test accuracy 78.00%\n",
      "Epoch 82: Train loss: 0.08585591311566532. Train accuracy 99.37%. Test accuracy 82.00%\n",
      "Epoch 83: Train loss: 0.08783728815615177. Train accuracy 99.68%. Test accuracy 82.00%\n",
      "Epoch 84: Train loss: 0.08593588415533304. Train accuracy 99.68%. Test accuracy 84.00%\n",
      "Epoch 85: Train loss: 0.06720465887337923. Train accuracy 99.37%. Test accuracy 83.00%\n",
      "Epoch 86: Train loss: 0.06494833342730999. Train accuracy 100.00%. Test accuracy 83.00%\n",
      "Epoch 87: Train loss: 0.07374214800074697. Train accuracy 99.68%. Test accuracy 80.00%\n",
      "Epoch 88: Train loss: 0.07083758106455207. Train accuracy 99.68%. Test accuracy 81.00%\n",
      "Epoch 89: Train loss: 0.08068005694076419. Train accuracy 100.00%. Test accuracy 80.00%\n",
      "Epoch 90: Train loss: 0.08431752678006887. Train accuracy 100.00%. Test accuracy 83.00%\n",
      "Epoch 91: Train loss: 0.12077682767994702. Train accuracy 98.73%. Test accuracy 78.00%\n",
      "Epoch 92: Train loss: 0.1101219579577446. Train accuracy 100.00%. Test accuracy 81.00%\n",
      "Epoch 93: Train loss: 0.11170104425400496. Train accuracy 99.37%. Test accuracy 81.00%\n",
      "Epoch 94: Train loss: 0.08382283663377166. Train accuracy 99.37%. Test accuracy 82.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: Train loss: 0.04881105991080403. Train accuracy 99.68%. Test accuracy 83.00%\n",
      "Epoch 96: Train loss: 0.08783674729056656. Train accuracy 100.00%. Test accuracy 80.00%\n",
      "Epoch 97: Train loss: 0.10056914947926998. Train accuracy 99.68%. Test accuracy 84.00%\n",
      "Epoch 98: Train loss: 0.08696066588163376. Train accuracy 99.37%. Test accuracy 82.00%\n",
      "Epoch 99: Train loss: 0.07340648770332336. Train accuracy 99.37%. Test accuracy 81.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_1D_BatchNorm(\n",
       "  (features): Sequential(\n",
       "    (0): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): Conv1d(28, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (4): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (5): PReLU(num_parameters=1)\n",
       "    (6): Dropout(p=0.1)\n",
       "    (7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (8): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (9): PReLU(num_parameters=1)\n",
       "    (10): Dropout(p=0.1)\n",
       "    (11): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (12): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (13): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (14): PReLU(num_parameters=1)\n",
       "    (15): Dropout(p=0.1)\n",
       "    (16): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (17): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (18): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): PReLU(num_parameters=1)\n",
       "    (20): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=416, out_features=250, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "    (2): Linear(in_features=250, out_features=2, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss(\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=100, batch_size=50, save_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.81, 0.83)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_te, y_te), model.load_model().score(X_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D dialated convolution + Batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([316, 28, 50]),\n",
       " torch.Size([316]),\n",
       " torch.Size([100, 28, 50]),\n",
       " torch.Size([100]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D_BatchNorm_Dial(modelWrapper):\n",
    "    def __init__(self, nb_hidden=50, activation=nn.ReLU):\n",
    "        torch.manual_seed(0) \n",
    "        super(CNN_1D_BatchNorm_Dial, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.nb_hidden = nb_hidden\n",
    "        \n",
    "        n_filters = 32\n",
    "        self.features = nn.Sequential(\n",
    "            nn.BatchNorm1d(28),\n",
    "            nn.Conv1d(28, n_filters, kernel_size=3, padding=2, dilation=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.Dropout(p=0.1),\n",
    "            \n",
    "            nn.BatchNorm1d(n_filters),\n",
    "            nn.Conv1d(n_filters, n_filters, kernel_size=3, padding=2, dilation=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(n_filters),\n",
    "            nn.Conv1d(n_filters, n_filters, kernel_size=5, padding=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.Dropout(p=0.1),\n",
    "            \n",
    "            nn.BatchNorm1d(n_filters),\n",
    "            nn.Conv1d(n_filters, n_filters, kernel_size=5, padding=4, dilation=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.Dropout(p=0.1),\n",
    "\n",
    "            nn.BatchNorm1d(n_filters),\n",
    "            nn.Conv1d(n_filters, 16, kernel_size=3, padding=1),\n",
    "            self.activation()\n",
    "        )\n",
    "        \n",
    "        self.num_features = 16*50\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features, nb_hidden),\n",
    "            self.activation(),  \n",
    "    \n",
    "            # (1, nb_hidden)\n",
    "            nn.Linear(nb_hidden, 2)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001, weight_decay=1e-1)\n",
    "#         self.optimizer = optim.Adagrad(self.parameters())\n",
    "#         self.optimizer = optim.Adamax(self.parameters())\n",
    "#         self.optimizer = optim.ASGD(self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_1D_BatchNorm_Dial(nb_hidden=50, activation=nn.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 0.4014696665108204. Train accuracy 96.84%. Test accuracy 78.00%\n",
      "Epoch 1: Train loss: 0.4478878416121006. Train accuracy 95.25%. Test accuracy 78.00%\n",
      "Epoch 2: Train loss: 0.4421656168997288. Train accuracy 97.78%. Test accuracy 78.00%\n",
      "Epoch 3: Train loss: 0.5841380655765533. Train accuracy 94.30%. Test accuracy 83.00%\n",
      "Epoch 4: Train loss: 0.8554799966514111. Train accuracy 93.99%. Test accuracy 72.00%\n",
      "Epoch 5: Train loss: 0.7371013388037682. Train accuracy 94.30%. Test accuracy 76.00%\n",
      "Epoch 6: Train loss: 0.7103092446923256. Train accuracy 93.04%. Test accuracy 69.00%\n",
      "Epoch 7: Train loss: 0.6791842952370644. Train accuracy 93.35%. Test accuracy 74.00%\n",
      "Epoch 8: Train loss: 0.7118119411170483. Train accuracy 93.99%. Test accuracy 68.00%\n",
      "Epoch 9: Train loss: 0.8483623340725899. Train accuracy 89.87%. Test accuracy 78.00%\n",
      "Epoch 10: Train loss: 0.7330001965165138. Train accuracy 95.89%. Test accuracy 68.00%\n",
      "Epoch 11: Train loss: 0.9778911098837852. Train accuracy 94.62%. Test accuracy 71.00%\n",
      "Epoch 12: Train loss: 0.764783188700676. Train accuracy 93.67%. Test accuracy 77.00%\n",
      "Epoch 13: Train loss: 0.8022929951548576. Train accuracy 93.99%. Test accuracy 76.00%\n",
      "Epoch 14: Train loss: 0.7457943335175514. Train accuracy 95.25%. Test accuracy 77.00%\n",
      "Epoch 15: Train loss: 0.7353012003004551. Train accuracy 94.30%. Test accuracy 77.00%\n",
      "Epoch 16: Train loss: 0.7837948054075241. Train accuracy 94.62%. Test accuracy 74.00%\n",
      "Epoch 17: Train loss: 0.7656443119049072. Train accuracy 93.35%. Test accuracy 78.00%\n",
      "Epoch 18: Train loss: 0.6372103728353977. Train accuracy 97.78%. Test accuracy 73.00%\n",
      "Epoch 19: Train loss: 0.5798203945159912. Train accuracy 97.47%. Test accuracy 77.00%\n",
      "Epoch 20: Train loss: 0.4172235578298569. Train accuracy 94.94%. Test accuracy 77.00%\n",
      "Epoch 21: Train loss: 0.40139859914779663. Train accuracy 98.42%. Test accuracy 80.00%\n",
      "Epoch 22: Train loss: 0.29912005737423897. Train accuracy 97.15%. Test accuracy 76.00%\n",
      "Epoch 23: Train loss: 0.3317418750375509. Train accuracy 97.47%. Test accuracy 77.00%\n",
      "Epoch 24: Train loss: 0.38420376740396023. Train accuracy 98.10%. Test accuracy 78.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_1D_BatchNorm(\n",
       "  (features): Sequential(\n",
       "    (0): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): Conv1d(28, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1)\n",
       "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "    (6): ReLU()\n",
       "    (7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (8): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (9): ReLU()\n",
       "    (10): Dropout(p=0.1)\n",
       "    (11): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (12): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.1)\n",
       "    (15): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (16): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (17): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=800, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss(\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=25, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D convolution + Batch norm (bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D_BatchNorm_Big(modelWrapper):\n",
    "    def __init__(self, nb_hidden=50, activation=nn.ReLU):\n",
    "        self.nb_hidden = nb_hidden\n",
    "        super(CNN_1D_BatchNorm_Big, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.BatchNorm1d(28),\n",
    "            nn.Conv1d(28, 32, kernel_size=5, padding=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.MaxPool1d(2, padding=1),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Conv1d(64, 32, kernel_size=5, padding=2),\n",
    "            nn.MaxPool1d(2, padding=1),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 32, kernel_size=5, padding=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 16, kernel_size=5, padding=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(16),\n",
    "        )\n",
    "        \n",
    "        self.num_features = 16*14\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features, nb_hidden),\n",
    "            self.activation(),\n",
    "            nn.Linear(nb_hidden, 2)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001, weight_decay=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_1D_BatchNorm_Big(activation=nn.PReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 11.170361161231995. Train accuracy 50.32%. Test accuracy 49.00%\n",
      "Epoch 1: Train loss: 10.62324196100235. Train accuracy 50.32%. Test accuracy 49.00%\n",
      "Epoch 2: Train loss: 10.209944903850555. Train accuracy 50.63%. Test accuracy 49.00%\n",
      "Epoch 3: Train loss: 9.325203478336334. Train accuracy 69.94%. Test accuracy 60.00%\n",
      "Epoch 4: Train loss: 8.580628991127014. Train accuracy 71.20%. Test accuracy 62.00%\n",
      "Epoch 5: Train loss: 8.552346497774124. Train accuracy 81.01%. Test accuracy 77.00%\n",
      "Epoch 6: Train loss: 7.921482741832733. Train accuracy 73.10%. Test accuracy 76.00%\n",
      "Epoch 7: Train loss: 6.992616504430771. Train accuracy 84.49%. Test accuracy 77.00%\n",
      "Epoch 8: Train loss: 5.614961341023445. Train accuracy 88.92%. Test accuracy 75.00%\n",
      "Epoch 9: Train loss: 5.244991287589073. Train accuracy 86.08%. Test accuracy 74.00%\n",
      "Epoch 10: Train loss: 5.9334060698747635. Train accuracy 79.43%. Test accuracy 75.00%\n",
      "Epoch 11: Train loss: 5.710421040654182. Train accuracy 83.54%. Test accuracy 76.00%\n",
      "Epoch 12: Train loss: 5.103396892547607. Train accuracy 79.75%. Test accuracy 68.00%\n",
      "Epoch 13: Train loss: 5.744888782501221. Train accuracy 88.61%. Test accuracy 77.00%\n",
      "Epoch 14: Train loss: 4.538559049367905. Train accuracy 90.51%. Test accuracy 68.00%\n",
      "Epoch 15: Train loss: 4.237046837806702. Train accuracy 88.61%. Test accuracy 77.00%\n",
      "Epoch 16: Train loss: 4.034591853618622. Train accuracy 86.71%. Test accuracy 78.00%\n",
      "Epoch 17: Train loss: 3.477476119995117. Train accuracy 87.66%. Test accuracy 64.00%\n",
      "Epoch 18: Train loss: 3.6481927931308746. Train accuracy 87.34%. Test accuracy 74.00%\n",
      "Epoch 19: Train loss: 3.892650783061981. Train accuracy 87.97%. Test accuracy 68.00%\n",
      "Epoch 20: Train loss: 3.9476601630449295. Train accuracy 89.24%. Test accuracy 77.00%\n",
      "Epoch 21: Train loss: 3.7158345580101013. Train accuracy 89.87%. Test accuracy 78.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-9b25c4a34b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/niccolo/Backup/Materiale_didattico/Deep learning/mini-projects/mini-project1/modelWrapper.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_test, y_test, batch_size, epochs, verbose, save_best_model)\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0;34m\"Epoch \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\": \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0;34m\"Train loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_loss_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\". \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0;34m'Train accuracy {:0.2f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\". \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                         ('Test accuracy {:0.2f}%'.format(self.score(X_test, y_test)*100) if compute_test_err else \"\"))\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/niccolo/Backup/Materiale_didattico/Deep learning/mini-projects/mini-project1/modelWrapper.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mtrue_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;31m# if one-hot encoding then extract the class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtrue_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/niccolo/Backup/Materiale_didattico/Deep learning/mini-projects/mini-project1/modelWrapper.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/niccolo/Backup/Materiale_didattico/Deep learning/mini-projects/mini-project1/modelWrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;34m\"\"\" Do the forward pass. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 168\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0m_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D convolution + Batch norm + Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual_block(nn.Module):\n",
    "    def __init__(self, activation=nn.ReLU):\n",
    "        super(residual_block, self).__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        num_filters = 32\n",
    "        self.features = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.Conv1d(num_filters, num_filters, kernel_size=3, padding=2, dilation=2),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.Conv1d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "            self.activation(),\n",
    "            \n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.Conv1d(num_filters, num_filters, kernel_size=3, padding=1),\n",
    "            self.activation(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        return x+self.features(x)\n",
    "\n",
    "class aggregated_residual_blocks(nn.Module):\n",
    "    def __init__(self, n_residual_blocks=2, activation=nn.ReLU):\n",
    "        super(aggregated_residual_blocks, self).__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "        for i in range(n_residual_blocks):\n",
    "            self.residual_blocks.append(residual_block(activation=activation))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        \n",
    "        for block in self.residual_blocks:\n",
    "            out.append(block(x))\n",
    "            \n",
    "        return sum(out)+x\n",
    "    \n",
    "class CNN_1D_BatchNorm_Residual(modelWrapper):\n",
    "    def __init__(self, nb_hidden=50, n_aggregated_residual_blocks=2, n_residual_blocks=2, activation=nn.ReLU):\n",
    "        # n_aggregated_residual_blocks: number of aggregated residual blocks (aggregated_residual_blocks)\n",
    "        # n_residual_blocks: number of residual blocks per aggregated residual block\n",
    "        \n",
    "        super(CNN_1D_BatchNorm_Residual, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.features = [\n",
    "            nn.BatchNorm1d(28),\n",
    "            nn.Conv1d(28, 32, kernel_size=3, padding=2, dilation=2),\n",
    "            self.activation()\n",
    "        ]\n",
    "        for i in range(n_aggregated_residual_blocks):\n",
    "            self.features.append(aggregated_residual_blocks(n_residual_blocks))\n",
    "            self.features.append(nn.Dropout(0.15))\n",
    "        \n",
    "        self.features += [            \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.MaxPool1d(2),\n",
    "            self.activation(),\n",
    "        ]\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "        \n",
    "        self.num_features = 16*25\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features, nb_hidden),\n",
    "            self.activation(),\n",
    "            nn.Linear(nb_hidden, 2)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adamax(self.parameters())\n",
    "#         self.optimizer = Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN_1D_BatchNorm_Residual(n_aggregated_residual_blocks=2, n_residual_blocks=5, activation=nn.ReLU)\n",
    "model = CNN_1D_BatchNorm_Residual(n_aggregated_residual_blocks=3, n_residual_blocks=1, activation=nn.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 11.243237972259521. Train accuracy 51.90%. Test accuracy 46.00%\n",
      "Epoch 1: Train loss: 11.041338443756104. Train accuracy 57.59%. Test accuracy 59.00%\n",
      "Epoch 2: Train loss: 10.867946028709412. Train accuracy 69.30%. Test accuracy 55.00%\n",
      "Epoch 3: Train loss: 10.594859302043915. Train accuracy 68.99%. Test accuracy 62.00%\n",
      "Epoch 4: Train loss: 9.839361727237701. Train accuracy 72.78%. Test accuracy 67.00%\n",
      "Epoch 5: Train loss: 8.814906239509583. Train accuracy 76.27%. Test accuracy 74.00%\n",
      "Epoch 6: Train loss: 7.4753225445747375. Train accuracy 82.91%. Test accuracy 76.00%\n",
      "Epoch 7: Train loss: 6.616357833147049. Train accuracy 83.23%. Test accuracy 73.00%\n",
      "Epoch 8: Train loss: 5.707961589097977. Train accuracy 85.13%. Test accuracy 76.00%\n",
      "Epoch 9: Train loss: 4.751588121056557. Train accuracy 88.92%. Test accuracy 68.00%\n",
      "Epoch 10: Train loss: 4.25025101006031. Train accuracy 89.56%. Test accuracy 76.00%\n",
      "Epoch 11: Train loss: 3.1224499940872192. Train accuracy 82.91%. Test accuracy 73.00%\n",
      "Epoch 12: Train loss: 3.19545878469944. Train accuracy 80.06%. Test accuracy 70.00%\n",
      "Epoch 13: Train loss: 3.7053719758987427. Train accuracy 89.56%. Test accuracy 74.00%\n",
      "Epoch 14: Train loss: 2.6492408215999603. Train accuracy 87.03%. Test accuracy 74.00%\n",
      "Epoch 15: Train loss: 2.3530716374516487. Train accuracy 89.56%. Test accuracy 76.00%\n",
      "Epoch 16: Train loss: 2.9540945142507553. Train accuracy 90.19%. Test accuracy 71.00%\n",
      "Epoch 17: Train loss: 3.0725568551570177. Train accuracy 91.77%. Test accuracy 74.00%\n",
      "Epoch 18: Train loss: 1.5874352790415287. Train accuracy 94.30%. Test accuracy 75.00%\n",
      "Epoch 19: Train loss: 1.0968046337366104. Train accuracy 95.25%. Test accuracy 73.00%\n",
      "Epoch 20: Train loss: 0.7900709239766002. Train accuracy 95.57%. Test accuracy 73.00%\n",
      "Epoch 21: Train loss: 0.7211486222222447. Train accuracy 97.15%. Test accuracy 73.00%\n",
      "Epoch 22: Train loss: 0.4390001422725618. Train accuracy 96.52%. Test accuracy 74.00%\n",
      "Epoch 23: Train loss: 0.3051479705609381. Train accuracy 96.52%. Test accuracy 72.00%\n",
      "Epoch 24: Train loss: 0.22712554945610464. Train accuracy 96.52%. Test accuracy 70.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_1D_BatchNorm_Residual(\n",
       "  (features): Sequential(\n",
       "    (0): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): Conv1d(28, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "    (2): ReLU()\n",
       "    (3): aggregated_residual_blocks(\n",
       "      (residual_blocks): ModuleList(\n",
       "        (0): residual_block(\n",
       "          (features): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (2): ReLU()\n",
       "            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (5): ReLU()\n",
       "            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (7): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (8): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Dropout(p=0.15)\n",
       "    (5): aggregated_residual_blocks(\n",
       "      (residual_blocks): ModuleList(\n",
       "        (0): residual_block(\n",
       "          (features): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (2): ReLU()\n",
       "            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (5): ReLU()\n",
       "            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (7): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (8): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Dropout(p=0.15)\n",
       "    (7): aggregated_residual_blocks(\n",
       "      (residual_blocks): ModuleList(\n",
       "        (0): residual_block(\n",
       "          (features): Sequential(\n",
       "            (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "            (2): ReLU()\n",
       "            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (5): ReLU()\n",
       "            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "            (7): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (8): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): Dropout(p=0.15)\n",
       "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (10): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (11): Dropout(p=0.1)\n",
       "    (12): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (13): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss(\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(modelWrapper):\n",
    "    def __init__(self, activation=nn.ReLU, nb_hidden=50):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D conv horizontal and 1D conv vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([316, 28, 50]),\n",
       " torch.Size([316]),\n",
       " torch.Size([100, 28, 50]),\n",
       " torch.Size([100]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, y_tr = Variable(train.X.clone()), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone()), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, in_channels, activation=nn.ReLU):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.activation = activation\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(in_channels)\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, padding=3) \n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, in_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (in_channels, 50)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(self.conv1(x))\n",
    "\n",
    "        # (64, 50)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation(self.conv2(x))\n",
    "\n",
    "        # (64, in_channels)\n",
    "        x = self.bn3(x)\n",
    "        x = self.activation(self.conv3(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "class conv2D(nn.Module):\n",
    "    def __init__(self, activation=nn.ReLU):\n",
    "        super(conv2D, self).__init__()\n",
    "        self.activation = activation\n",
    "                \n",
    "        self.bn1 = nn.BatchNorm1d(1)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 7), padding=(1, 3))\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 16, kernel_size=5, padding=2)\n",
    "#         self.conv4 = nn.Conv2d(32, 16, kernel_size=5, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (1, 28, 50) -> (32, 14, 26)\n",
    "        x = self.activation(F.max_pool2d(self.conv1(self.bn1(x)), 2, padding=(0, 1)))\n",
    "\n",
    "        # (32, 14, 26) -> (32, 8, 14)\n",
    "        x = self.activation(F.max_pool2d(self.conv2(self.bn2(x)), (2, 2), padding=(1, 1)))\n",
    "\n",
    "        # (32, 8, 14) -> (16, 4, 7)\n",
    "        x = self.activation(F.max_pool2d(self.conv3(self.bn3(x)), 2))\n",
    "\n",
    "        return x\n",
    "        \n",
    "class horiz_vert_1D(modelWrapper):\n",
    "    def __init__(self, activation=nn.ReLU, nb_hidden=50):\n",
    "        super(horiz_vert_1D, self).__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.horiz = CNN_1D(28, activation=activation)\n",
    "        self.vert = CNN_1D(50, activation=activation)\n",
    "        \n",
    "        self.conv2D = conv2D()\n",
    "        self.conv2D_horiz = conv2D()\n",
    "        self.conv2D_vert = conv2D()\n",
    "        \n",
    "        self.fc1 = nn.Linear(448, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.conv2D_horiz(self.horiz(x).unsqueeze(1))\n",
    "        out2 = self.conv2D_vert(self.vert(x.transpose(1, 2)).transpose(1, 2).unsqueeze(1))\n",
    "        out3 = self.conv2D(x.unsqueeze(1))\n",
    "        \n",
    "        out = out1 + out2 + out3\n",
    "        \n",
    "        out = self.activation(self.fc1(out.view(-1, 448))) \n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = horiz_vert_1D(nb_hidden=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual network with 2D convolutional layers + batch normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a channel to store the pixels image (so to apply the 2D convolutional layer)\n",
    "X_tr, y_tr = Variable(train.X.clone().unsqueeze(1)), Variable(train.y)\n",
    "X_te, y_te = Variable(test.X.clone().unsqueeze(1)), Variable(test.y)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resudial network\n",
    "class BasicBlock():\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "    \n",
    "class NetworkBlock():\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(, modelWrapper):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = int((depth - 4) / 6)\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(1, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        #self.nChannels = nChannels[3]\n",
    "        self.fc = nn.Linear(1152, num_classes)\n",
    "        self.nChannels = 1152\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "#                 m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.weight.data.fill_(1)\n",
    "#                 m.bias.data.zero_()\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        out = self.conv1(x)\n",
    "        #print(out.size())\n",
    "        out = self.block1(out)\n",
    "        #print(out.size())\n",
    "        out = self.block2(out)\n",
    "        #print(out.size())\n",
    "        out = self.block3(out)\n",
    "        #print(out.size())\n",
    "        out = self.relu(self.bn1(out))\n",
    "        #print(out.size())\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        #print(out.size())\n",
    "        out = self.fc(out.view(-1, self.nChannels))\n",
    "        #print(out.size())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(depth=16, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr, y_tr, X_test=X_te, y_test=y_te, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
